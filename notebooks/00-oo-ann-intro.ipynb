{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit of history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neural Networks have been around for quite a while. They have been first introduced by Warren McCulloch and Walter Pitts: \n",
    "\n",
    "[A Logical Calculus Of The Ideas Immanent In Nervous Activity](https://link.springer.com/article/10.1007/BF02478259).\n",
    "\n",
    "The paper presented a simplified computational model of how biological neurons might work together in animal brains.\n",
    "\n",
    "There where early successes until the 1960s and it was believed that we would soon be able to converse with truly intelligent machines. When it turned out that this wouldn't be the case, ANNs entered a dark age.\n",
    "\n",
    "There was a short burst of interest again in the 1980s, caused by new architectures and improved training machines. But by 1990 the interest died down again, instead other, powerful ML techniques like Support Vector Machines were favored.\n",
    "\n",
    "There has been a surge of interest in ANNs for a few years now, among other things caused by huge datasets available for training these days and the tremendous increase in computing power, mostly caused by the gaming industry and the development of powerful GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very first models for artificial neurons were very simple: A neuron has one or more binary inputs and one binary output. The neuron activates its output when more than a certain number of its inputs are active.\n",
    "\n",
    "Even with such a simple model one can build a neural network that can compute any logical proposition you want.\n",
    "\n",
    "![some simple boolean operations expressed in a neural network](img/ann-bool.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LTU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slightly modified version of an artificial neuron is the linear threshold unit (LTU). Inputs and outputs are now numbers instead of binary values.\n",
    "\n",
    "Each input is now associated with a weight. The LTU computes a weighted sum of its inputs, then applies a step function to that sum and outputs the result.\n",
    "\n",
    "Common step functions used in preceptrons are heaviside functions or sign functions.\n",
    "\n",
    "![LTU](img/ltu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron is a very simply ANN architecture that is based on the LTU.\n",
    "\n",
    "It was invented in 1957 by Frank Rosenblatt.\n",
    "\n",
    "The most common step function used in preceptrons is the Heavyside Step Function. Sometimes the sign function is used instead.\n",
    "\n",
    "A single LTU can be used for binary classification by computing a linear combination of weights. Once the threshold is exceeded the output is positive.\n",
    "\n",
    "A Perceptron is composed of a single layer of neurons, with each neuron connected to all inputs. An extra bias feature (remember, each input is a feature) is represented by a bias neuron that just simply outputs 1 all the time.\n",
    "\n",
    "![perceptron with two outputs](img/perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Multi-Layer Perceptron is sort of a logistic regression classifier. The input is first transformed using a learned non-linear transformation.\n",
    "\n",
    "The transformation is being projected into a space where it becomes linearly seperable. This space is referred to as a hidden layer. A single hidden layer is enough to make MLPs a universal approximator. Adding more hidden layers bring a lot of benefits tho and are the premise for deep learning. So, it is useful to know about MLPs before diving into deep learning. \n",
    "\n",
    "![mlp](img/mlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is one of plenty examples from keras on how to use an MLP on the MNIST dataset: https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py\n",
    "\n",
    "The keras documentation is seriously awesome imo, I can really recommend browsing through it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
